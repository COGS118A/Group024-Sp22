{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Drake Coy\n",
    "- Aditi Krishnakumar\n",
    "- Simran Nayyer\n",
    "- Jasmine Qiang\n",
    "- Luke Sztajnkrycer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our project is centered around accurately predicting the price of flights to find the best booking times and routes. Our goal is to enable customers to plan their trips as efficiently and cost effective as possible. We will examine data collected from ‘Easemytrip,’ an online booking platform, between February 11th and March 31st of 2022 that includes flight information from India about airline companies, flight codes, source/destination cities, flight times, and more. There are 12 total features that mostly take the form of discrete categorical variables (‘Delhi,’ ‘morning,’ ‘economy,’ etc.), and includes 300,000 unique observations. We plan on implementing both a decision tree and linear regression model to make our predictions. Each method will be compared to evaluate which provides a higher success rate. We will measure the performance of our linear regression model by utilizing R-squared, adjusted R-squared, mean absolute error metrics to evaluate. We will then evaluate our decision tree success rate by using K-means cross validation (using ~⅕ of the data as a test set) to determine how generalizable our decision criteria are to any given portion of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Groves & Gini used a standard PLS regression to construct their model, as well as lagged feature computation to weight temporal relationships in the data. <a name=\"groves\"></a>[<sup>[1]</sup>](#grovesnote)\n",
    "\n",
    "Wohlfarth et al. used a marked point process of returns to create a piecewise reconstruction of the price trajectory, which they then used to create an intensity image to perform clustering, feature-based classification, and model generation. <a name=\"Wohlfarth\"></a>[<sup>[2]</sup>](#Wohlfarthnote)\n",
    "\n",
    "Guo & Luh used a committee machine of neural networks, combining RBF and MLP networks to balance local data characteristics with global trends. <a name=\"Guo\"></a>[<sup>[3]</sup>](#Guonote)\n",
    "\n",
    "Etzioni et al. compared reinforcement learning and rule learning, where the system determined whether to buy or wait for each instance of a ticket price. They also compared those with a moving average model to predict the price of a flight based on its own history. <a name=\"Etzioni\"></a>[<sup>[4]</sup>](#Etzioninote)\n",
    "\n",
    "In general, most of the prior work seems to agree that for this task, it is important to choose a model that considers more general, long-term patterns as opposed to finer details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "When booking flights, there is often a tradeoff between aspects such as comfort (in terms of the airline) and or time spent (duration of the flight) that make the experience uncomfortable in the name of saving money. Our team intends to give insight into what factors influence flight pricing the most so that when readers of our work are booking flights in the future, they will be able to prioritize these factors in order to minimize their flight price. We plan to provide this service through linear regression by establishing relationships between flight price and other factors and through a decision tree by combining various results of our regression to categorize flights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction\n",
    "\n",
    "Our dataset contains flight data to six metropolitan cities in India from Easemytrip which is an online platform for booking plane tickets. There are 300,261 observations in this dataset and 11 variables per observation. The 11 features are as follows: airline, flight (number), source city, departure time, stops, arrival time, destination city, class,  duration, days left (from the booking date till fight date), and price. Some critical variables from this list include the airline, days left, destination city, and stops as we think that these variables logically should have the most influence on the price variable. The dataset seems to be cleaned already but depending on how we aim to explore the analysis, we may need to one-hot encode certain categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "There is no true way to determine the best tradeoff between comfort, price, and time when it comes to flights. This is primarily due to the fact that individuals have different preferences, available time, amounts of money, etc. Thus, we opt for a solution that takes into account all of these variables, and more, to arrange groups of flights/tickets that have a broad appeal to different consumer groups. For example, we may group a set of flights that share characteristics of low price, medium-to-long flight time, and free luggage booking to cater to individuals who like to travel often and cheaply. We may group another set of flights based on quality of service and amount of reroutes to cater to individuals with greater amounts of money who prefer nice, one-and-done flights.\n",
    "\n",
    "Notably, while we have some broad ideas of consumer/ticket groups in mind (such as the examples described above), we aim to construct these groups analytically, with variable contents and cutoff (e.g. tickets underneath x price, trip has less than x connecting flights) determined by their relationship to other variables in this dataset. As of now, the best way we believe we can approach this is through Linear Regression. Not only can we use regression to find relationships between our variables, but we can use these relationships to construct ideal ticket categories that appeal to a broad range of people. The fit of the relationship between any two variables can be described by the R-squared metric.\n",
    "\n",
    "Once we have constructed these ticket categories, we will program a decision tree that classifies each flight by their assigned categories. This should give us a better idea of how each flight grouping is oriented in space (though we may have to resort to dimensionality-reducing methods such as PCA or scrap this visualization if the groups are too hyper-dimensional), and provide us with a simple-yet-effective set of criteria that allow us to arrive at any given group. In the real world, this might translate to a customer being offered a range of tickets that suit their preferences after answering a short list of questions. The shorter the list and the better the accuracy of each group, the better, which means we desire to make our tree as simple and accurate as possible. We plan to use K-means cross validation to ensure that our model is generalizable to any given portion of the dataset (since we likely do not have other datasets to work with, we can treat this set as 5-10 smaller datasets), and we are also considering comparing our performance to other decision tree models specified in lecture (assuming they are not copyrighted). To make the best tree we can, we plan to use GridSearchCV (or a related function, if there is a more applicable one) with a param grid to test how changing hyper-parameters and their parameters affects our tree.\n",
    "\n",
    "Additionally, we will try operationalizing the same idea listed above using random forests instead of decision trees. We will again use a parameter grid to select both parameters and hyperparameters for the model.\n",
    "\n",
    "All data cleaning, modeling, and visualization will likely be done using a combination of functions from the Pandas, NumPy, and Scikit-Learn libraries. We will create our own functions when needed and specify clearly which functions we use in our final project submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since we are using a regression model to predict continuous data (price), our primary evaluation metric will be an R Square Score which indicates the percentage of variance in the dependent variable that is explained by the independent variables (i.e. how well the model fits our dependent variable (price)). It is a score between 0 and 1 where 1 indicates a perfect fit between predicted and actual values. \n",
    "\n",
    "**ADD IMAGE**\n",
    "The numerator contains the sum of the residuals squared and the denominator is the sum squared of the distance between the data and the mean.\n",
    "\n",
    "Since R Square does not account for the problem of overfitting, we will additionally look at Adjusted R Square which penalizes additional independent variables added to the model and adjusts the evaluation metric to account for overfitting. It is always less than or equal to the R Square score. \n",
    "\n",
    "\n",
    "N is the number of sample data points and k is the number of independent variables in the model.\n",
    "\n",
    "We will supplement our evaluation with Mean Absolute Error as another evaluation metric. This will help us understand how far off our price predictions are from the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen this particular dataset as it minimizes the exposure of personally identifiable information, containing only details about the airline itself. Furthermore, this data is sourced from a publicly available website ‘Easemytrip’ and published on Kaggle, which provides users consent to explore, publish, and access the information. \n",
    "\n",
    "\n",
    "Our model itself is used to predict the price alone and nothing that may cause ethical concerns once in production such as the specific flight route, passenger details, etc. Additionally, the data is based on previous flight routes alone. \n",
    "\n",
    "\n",
    "We do acknowledge the possible bias in our dataset because all the flight routes are within a single country (India). This is a limitation in the generalizability of our model and will therefore limit its uses in production. We will ensure that this bias is highlighted to the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE THE PROPOSAL TIMELINE ACCORDING TO WHAT HAS ACTUALLY HAPPENED AND HOW IT HAS EFFECTED YOUR FUTURE PLANS\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic (Pelé) | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets (Beckenbaur)  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data ,do some EDA (Maradonna) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
